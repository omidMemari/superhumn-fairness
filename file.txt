"""
    optim1 = optim.Adam(list(self.model_obj.parameters()), lr=0.1)
    optim2 = optim.Adam([self.alpha], lr=0.1)
    for i in range(1):
      print("epoch:", i)
      self.sample_superhuman()
      self.get_samples_demo_indexed()
      self.get_sample_loss()
      loss = self.subdom_loss_t()
      optim2.zero_grad()
      optim1.zero_grad()
      (loss).backward()
      optim1.step()
      optim2.step()
      print(self.alpha)
      losses.append(loss.item())
      print("loss: ", loss.item())
      self.pred_scores = self.model_obj(X_test)
      print(self.pred_scores)
      # new_alpha = self.compute_alpha()
      # self.update_model_alpha(new_alpha)
    # write losses to file
    training loop
    """